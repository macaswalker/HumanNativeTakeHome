{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-Home Assignment - Human Native - Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are the ML team at Human Native. We want to build a system that reduces the workload of the operations team: this team curretly reviews hundreds of thousand of media (multimedia: text, image, audio, video, animation).\n",
    "\n",
    "The engineering team currently has a set up like so: \n",
    "\n",
    "```\n",
    "Dataset(org_id, id, name, type)\n",
    "Data(dataset_id, id, value, flag)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a model that automatically flags data that might be in violation.\n",
    "\n",
    "We are doing this to be able to provide a service that will allow consumrs to provide structured information about where, why and how the data is in violation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, what this small MVP project wishes to show is that:\n",
    "1. for a specific modality of data (we will use text)\n",
    "2. we can return structured information about where, why and how the data is in violation\n",
    "3. we want to build a model that does this\n",
    "4. we also want to provide recommendations about the current data structures used by the engineering team to make this problem as easy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some specifications:\n",
    "- we will note our assumptions as we go along\n",
    "- we want to build our own model, where we can\n",
    "- we don't need to worry about persistence\n",
    "- our model currently has access to all previously flagged data items as well as those verified as corrct by the operations team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Plan \n",
    "\n",
    "We will design a pipeline that is independent of the model we use to classify. We will do this to rapidly iterate. We will then train our own distilbert model for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP of an MVP\n",
    "\n",
    "Here, we will simply take in a bunch of text of we want to build something that returns an object: this object contains the original text, the text with redactions, structured information about where the PII is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8m/mlfdjz4d5bd1hd67c6nkhb_00000gn/T/ipykernel_873/2346554238.py\", line 2, in <module>\n",
      "    from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/__init__.py\", line 9, in <module>\n",
      "    from presidio_analyzer.entity_recognizer import EntityRecognizer\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/entity_recognizer.py\", line 6, in <module>\n",
      "    from presidio_analyzer.nlp_engine import NlpArtifacts\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/nlp_engine/__init__.py\", line 4, in <module>\n",
      "    from .nlp_artifacts import NlpArtifacts\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/nlp_engine/nlp_artifacts.py\", line 4, in <module>\n",
      "    from spacy.tokens import Doc, Span\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: LOCATION, start: 80, end: 97, score: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Relevant imports\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "sample_text = \"hello my name is  Mac Walker, my email is macskyewalker@gmail.com and I live at 10 downing street\"\n",
    "sample_entities = [\"PHONE_NUMBER\", \"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"UK_NINO\"]\n",
    "\n",
    "# Please see a full list of supported entities here: (https://microsoft.github.io/presidio/supported_entities/)\n",
    "\n",
    "\n",
    "analyser = AnalyzerEngine()\n",
    "\n",
    "analyser_results = analyser.analyze(text=sample_text, entities = sample_entities, language='en')\n",
    "\n",
    "print((analyser_results[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we now have an extremely basic system that can detect PII. Right now, this is a tool, so lets build functionality around it to actually go through our datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: EMAIL_ADDRESS, start: 42, end: 65, score: 1.0, type: PERSON, start: 18, end: 28, score: 0.85, type: LOCATION, start: 80, end: 97, score: 0.85]\n"
     ]
    }
   ],
   "source": [
    "analyser = AnalyzerEngine()\n",
    "\n",
    "def presidio_detect_PII(text, entities, language):\n",
    "\n",
    "    assert isinstance(text, str), \"text must be a string\"\n",
    "    assert isinstance(entities, list) and all(isinstance(e, str) for e in entities), \"entities must be a list of strings\"\n",
    "    assert isinstance(language, str), \"language must be a string\"\n",
    "\n",
    "    analyser_results = analyser.analyze(text=text, entities=entities, language=language)\n",
    "    return analyser_results\n",
    "\n",
    "# Small Test\n",
    "sample_results = presidio_detect_PII(text= sample_text, entities = sample_entities, language = 'en')\n",
    "print(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: hello my name is  [PERSON], my email is [EMAIL_ADDRESS] and I live at [LOCATION]\n",
      "items:\n",
      "[\n",
      "    {'start': 70, 'end': 80, 'entity_type': 'LOCATION', 'text': '[LOCATION]', 'operator': 'replace'},\n",
      "    {'start': 40, 'end': 55, 'entity_type': 'EMAIL_ADDRESS', 'text': '[EMAIL_ADDRESS]', 'operator': 'replace'},\n",
      "    {'start': 18, 'end': 26, 'entity_type': 'PERSON', 'text': '[PERSON]', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anonymiser = AnonymizerEngine()\n",
    "def presidio_replace_PII(text, entities, language):\n",
    "\n",
    "    analyzer_results = presidio_detect_PII(text = text, entities = entities,language=language)\n",
    "\n",
    "    operators = {\n",
    "            result.entity_type: OperatorConfig(\"replace\", {\"new_value\": f\"[{result.entity_type}]\"}) for result in analyser_results\n",
    "        } # got this from ChatGPT\n",
    "\n",
    "    anonymised_results = anonymiser.anonymize(\n",
    "        text = text,\n",
    "        analyzer_results = analyzer_results,\n",
    "        operators = operators\n",
    "        \n",
    "    )\n",
    "    return anonymised_results\n",
    "\n",
    "#Small Test\n",
    "sample_anonymised = presidio_replace_PII(text= sample_text, entities = sample_entities, language = 'en')\n",
    "print(sample_anonymised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we have some functions that allow us to take in a piece of text, detect where the PII is, and return the text that has been anonymised.\n",
    "\n",
    "Right now, we want to test two models. We have our approach above which isn't ours, and a few more ML approaches for models we can train. \n",
    "\n",
    "The next step, before training our model, should be construction of a relatively big dataset (1000 blog post entries), so we can start with our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faker\n",
    "\n",
    "def generate_blogpost(pii_mode):\n",
    "    base_sentences = faker.paragraph(nb_sentences=6).split('. ')\n",
    "    flag = 1 if pii_mode != \"none\" else 0\n",
    "    pii_metadata = []\n",
    "\n",
    "    pii_snippets = [\n",
    "        (\"NAME\", faker.name),\n",
    "        (\"EMAIL\", faker.email),\n",
    "        (\"LOCATION\", faker.city),\n",
    "        (\"PHONE\", faker.phone_number),\n",
    "        (\"URL\", lambda: f\"https://{faker.domain_name()}\"),\n",
    "    ]\n",
    "\n",
    "    if pii_mode == \"none\":\n",
    "        full_text = '. '.join(base_sentences).strip()\n",
    "        return full_text, flag, pii_mode, pii_metadata\n",
    "\n",
    "    pii_type, pii_func = random.choice(pii_snippets)\n",
    "    pii_value = pii_func()\n",
    "\n",
    "    if pii_mode == \"standalone\":\n",
    "        insert_idx = random.randint(0, len(base_sentences))\n",
    "        base_sentences.insert(insert_idx, pii_value)\n",
    "\n",
    "    elif pii_mode == \"raw\":\n",
    "        idx = random.randint(0, len(base_sentences) - 1)\n",
    "        base_sentences[idx] += f\". {pii_value}\"\n",
    "\n",
    "    elif pii_mode == \"embedded\":\n",
    "        embedded_templates = [\n",
    "            (\"NAME\", f\"my name is {faker.name()}\"),\n",
    "            (\"EMAIL\", f\"you can email me at {faker.email()}\"),\n",
    "            (\"LOCATION\", f\"I live in {faker.city()}\"),\n",
    "            (\"PHONE\", f\"my number is {faker.phone_number()}\"),\n",
    "            (\"URL\", f\"visit my site at https://{faker.domain_name()}\"),\n",
    "        ]\n",
    "        pii_type, pii_text = random.choice(embedded_templates)\n",
    "        idx = random.randint(0, len(base_sentences) - 1)\n",
    "        if random.random() < 0.5:\n",
    "            base_sentences[idx] = pii_text + '. ' + base_sentences[idx]\n",
    "        else:\n",
    "            base_sentences[idx] += '. ' + pii_text\n",
    "        pii_value = pii_text  # overwrite for consistency\n",
    "\n",
    "    # Join final text and record location of the PII span\n",
    "    full_text = '. '.join(base_sentences).strip()\n",
    "\n",
    "    if pii_value in full_text:\n",
    "        start = full_text.index(pii_value)\n",
    "        end = start + len(pii_value)\n",
    "        pii_metadata.append({\n",
    "            \"type\": pii_type,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"value\": pii_value\n",
    "        })\n",
    "\n",
    "    return full_text, flag, pii_mode, pii_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the above was created using ChatGPT - it did this in like 5 minutes of discussion! This is great though. Let's go over what we currently have:\n",
    "\n",
    "Dataset(ord_ig, id, name, type): I haven't 'created' this yet, because we basically only have 1 entry to it (which is what we will be using to train our model). \n",
    " - ```Dataset(faker_id: (haven't made one), faker_blog_posts_id : a5c85cbf-c960-431f-b2e5-d4ba1a9601b9, faker_blog_posts, text)```\n",
    "\n",
    "\n",
    "Data(dataset_id, id, value, flag):\n",
    "- ```Data(a5c85cbf-c960-431f-b2e5-d4ba1a9601b9,bbb0d0ec-ec58-4f68-8047-692a59edfd44, \"Value idea trade left. Bit practice already billion call degree. Tax professor mission stock because. Ahead each fish onto.\", 0, none)```\n",
    "\n",
    "So we have an example of our Data and our Dataset. Let us make some observations about our data. There are a few.\n",
    "\n",
    "---------- \n",
    "\n",
    "1. the naming of ```org_id, id, dataset_id and id``` (again?!!!) is confusing to me. Hence, I recommend changing the id naming convention to maintain readability, to this: \n",
    "- ```Dataset(org_id, id, name, type)``` -> ```Dataset(org_id, dataset_id, name, type)```\n",
    "- ```Data(dataset_id, id, value, flag)``` -> ```Data(dataset_id, data_id, value, flag)```\n",
    "\n",
    "2. We have the value, which is basically the data values (text, images, animation etc). We also have a flag which supposedly tells us whether it contains PII? We should actually include WHERE in the content the PII is, WHAT the PII is (what has been detected) and HOW we have dealt with it (have we removed? anonymised? encrypted?)\n",
    "\n",
    "3. We currently have a flag. But where is that flag from? Is it from the operations team who have flagged this? or is it from our model? we should keep track of which model gave it whcih flag so we can continuously check against the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "dataset_id = str(uuid.uuid4())\n",
    "data_rows = []\n",
    "\n",
    "for mode in [\"embedded\", \"raw\", \"standalone\", \"none\"]:\n",
    "    for _ in range(250):\n",
    "        text, flag, pii_mode, pii_spans = generate_blogpost(pii_mode=mode)\n",
    "        data_rows.append({\n",
    "            'dataset_id': dataset_id,\n",
    "            'id': str(uuid4()),\n",
    "            'value': text,\n",
    "            'flag': flag,\n",
    "            'pii_mode': pii_mode,\n",
    "            'pii_spans': json.dumps(pii_spans)\n",
    "        })\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(data_rows)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(\"Data_with_spans.csv\", index=False)\n",
    "df.to_json(\"Data_with_spans.json\", orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting PII with Presidio: 100%|██████████| 1000/1000 [00:19<00:00, 50.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 paragraphs: 19.95 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans.csv\")\n",
    "\n",
    "sample_entities = [\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\", \"LOCATION\", \"URL\"]\n",
    "\n",
    "analyser = AnalyzerEngine()\n",
    "\n",
    "start = time.time()\n",
    "found_flags = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Detecting PII with Presidio\"):\n",
    "    text = row[\"value\"]\n",
    "    results = analyser.analyze(text=text, entities=sample_entities, language=\"en\")\n",
    "    found_flags.append(int(len(results) > 0))\n",
    "\n",
    "df[\"found_flag\"] = found_flags\n",
    "df.to_csv(\"Data_with_spans_and_found_flag.csv\", index=False)\n",
    "\n",
    "print(f\"Time taken for 1000 paragraphs: {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      no_pii       0.75      0.92      0.83       250\n",
      "     has_pii       0.97      0.90      0.93       750\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.86      0.91      0.88      1000\n",
      "weighted avg       0.92      0.90      0.91      1000\n",
      "\n",
      "\n",
      "🔄 Confusion Matrix:\n",
      "[[230  20]\n",
      " [ 76 674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans_and_found_flag.csv\")\n",
    "\n",
    "print(\"\\n📊 Overall Classification Report:\")\n",
    "print(classification_report(df[\"flag\"], df[\"found_flag\"], target_names=[\"no_pii\", \"has_pii\"]))\n",
    "\n",
    "print(\"\\n🔄 Confusion Matrix:\")\n",
    "print(confusion_matrix(df[\"flag\"], df[\"found_flag\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Accuracy by PII Mode:\n",
      "\n",
      "Embedded    : Accuracy = 0.920 (250 samples)\n",
      "None        : Accuracy = 0.920 (250 samples)\n",
      "Raw         : Accuracy = 0.884 (250 samples)\n",
      "Standalone  : Accuracy = 0.892 (250 samples)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans_and_found_flag.csv\")\n",
    "\n",
    "print(\"\\n📁 Accuracy by PII Mode:\\n\")\n",
    "\n",
    "for mode in sorted(df[\"pii_mode\"].unique()):\n",
    "    subset = df[df[\"pii_mode\"] == mode]\n",
    "    y_true = subset[\"flag\"]\n",
    "    y_pred = subset[\"found_flag\"]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"{mode.title():<12}: Accuracy = {acc:.3f} ({len(subset)} samples)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so lets think about what we have - a _baseline_.\n",
    "\n",
    "This, in my opinion, is a really useful thing to have. It is all well and good fine-tuning a huge transformer model for PII detection (which we will proceed to do), but we should understand _why_ we are doing it and what benefits it will give this process.\n",
    "\n",
    "Our Presidio-only model (which actually uses a transformer sneakily in the back) achieves an accuracy of 92%. This is okay - not amazing. \n",
    "When inspecing further, we can see a difference between the accuracies of the four different types of PII injection we used to create the dummy dataset. Our sample size is too small to make too many conclusions, apart from we can maybe say that the model is better at classifying `None` and `Embedded` relative to `Raw` and `Standalone`. \n",
    "\n",
    "I posit that the reason for this is that for both `None` and `Embedded`, these are more \n",
    "\n",
    "\n",
    "FINISH LATER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing training data: 1000it [00:00, 4360.16it/s]\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1:  87%|████████▋ | 87/100 [04:52<00:43,  3.37s/it, loss=0.0476]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    173\u001b[39m     optimizer.zero_grad()\n\u001b[32m    174\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m})\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/optim/adamw.py:187\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    174\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    176\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    177\u001b[39m         group,\n\u001b[32m    178\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m         state_steps,\n\u001b[32m    185\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/optim/adamw.py:339\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    337\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/optim/adamw.py:419\u001b[39m, in \u001b[36m_single_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    418\u001b[39m exp_avg.lerp_(grad, \u001b[32m1\u001b[39m - beta1)\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    422\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Define the PII types and tag scheme\n",
    "tag2id = {\n",
    "    'O': 0,       # Not a PII token\n",
    "    'B-NAME': 1,  # Beginning of NAME entity\n",
    "    'I-NAME': 2,  # Inside of NAME entity \n",
    "    'B-EMAIL': 3, # Beginning of EMAIL entity\n",
    "    'I-EMAIL': 4, # Inside of EMAIL entity\n",
    "    'B-PHONE': 5, # Beginning of PHONE entity\n",
    "    'I-PHONE': 6, # Inside of PHONE entity\n",
    "    'B-LOCATION': 7, # Beginning of LOCATION entity\n",
    "    'I-LOCATION': 8, # Inside of LOCATION entity\n",
    "    'B-URL': 9,   # Beginning of URL entity\n",
    "    'I-URL': 10   # Inside of URL entity\n",
    "}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "# Step 2: Load and prepare the data\n",
    "df = pd.read_csv(\"Data_with_spans.csv\")\n",
    "\n",
    "# Function to convert spans to token-level labels\n",
    "def create_token_labels(text, pii_spans):\n",
    "    \"\"\"Convert PII spans to token level labels.\"\"\"\n",
    "    spans = json.loads(pii_spans) if isinstance(pii_spans, str) else pii_spans\n",
    "    \n",
    "    # Initialize with 'O' (Outside) labels\n",
    "    tokens = text.split()\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Calculate token boundaries\n",
    "    token_boundaries = []\n",
    "    current_pos = 0\n",
    "    for token in tokens:\n",
    "        token_start = text.find(token, current_pos)\n",
    "        token_end = token_start + len(token)\n",
    "        token_boundaries.append((token_start, token_end))\n",
    "        current_pos = token_end\n",
    "    \n",
    "    # Assign labels based on spans\n",
    "    for span in spans:\n",
    "        span_start = span['start']\n",
    "        span_end = span['end']\n",
    "        span_type = span['type']\n",
    "        \n",
    "        for i, (token_start, token_end) in enumerate(token_boundaries):\n",
    "            # Token is at the start of the span\n",
    "            if token_start <= span_start < token_end:\n",
    "                labels[i] = f'B-{span_type}'\n",
    "            # Token is inside the span\n",
    "            elif span_start <= token_start < span_end:\n",
    "                labels[i] = f'I-{span_type}'\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "# Prepare dataset with token labels\n",
    "train_data = []\n",
    "for _, row in tqdm(df.iterrows(), desc=\"Preparing training data\"):\n",
    "    if row['flag'] == 1:  # Only process rows with PII\n",
    "        tokens, labels = create_token_labels(row['value'], row['pii_spans'])\n",
    "        train_data.append({\n",
    "            'tokens': tokens,\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "# Add negative examples (no PII)\n",
    "for _, row in df[df['flag'] == 0].head(len(train_data) // 2).iterrows():\n",
    "    tokens = row['value'].split()\n",
    "    labels = ['O'] * len(tokens)\n",
    "    train_data.append({\n",
    "        'tokens': tokens,\n",
    "        'labels': labels\n",
    "    })\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_dataset, val_dataset = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create a dataset class for token classification\n",
    "class PIITokenDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['tokens']\n",
    "        token_labels = item['labels']\n",
    "        \n",
    "        # Tokenize each token - this handles subword tokenization\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create token type labels\n",
    "        labels = torch.ones(tokenized_inputs['input_ids'].shape, dtype=torch.long) * -100\n",
    "        \n",
    "        # Align tokens with labels\n",
    "        word_ids = tokenized_inputs.word_ids()\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                # Use the first subword of each word for the label\n",
    "                if i > 0 and word_ids[i-1] == word_idx:\n",
    "                    labels[0, i] = -100  # Ignore other subword tokens\n",
    "                else:\n",
    "                    tag = token_labels[word_idx]\n",
    "                    labels[0, i] = tag2id[tag]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized_inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n",
    "\n",
    "# Step 4: Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=len(tag2id)\n",
    ")\n",
    "\n",
    "# Step 5: Create data loaders\n",
    "train_dataset_encoded = PIITokenDataset(train_dataset, tokenizer)\n",
    "val_dataset_encoded = PIITokenDataset(val_dataset, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset_encoded, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_encoded, batch_size=8)\n",
    "\n",
    "# Step 6: Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions for metrics\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Only keep predictions for non-padding tokens (labels != -100)\n",
    "            for i in range(labels.shape[0]):\n",
    "                for j in range(labels.shape[1]):\n",
    "                    if labels[i, j] != -100:\n",
    "                        true_labels.append(labels[i, j].item())\n",
    "                        predictions.append(pred[i, j].item())\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    # Count correct predictions\n",
    "    correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)\n",
    "    print(f\"Accuracy: {correct/len(true_labels):.4f}\")\n",
    "    \n",
    "    # Count by entity type\n",
    "    entity_correct = {}\n",
    "    entity_total = {}\n",
    "    \n",
    "    for pred, true in zip(predictions, true_labels):\n",
    "        pred_tag = id2tag[pred]\n",
    "        true_tag = id2tag[true]\n",
    "        \n",
    "        if true_tag != 'O':  # Only count PII tokens\n",
    "            entity_type = true_tag.split('-')[1] if '-' in true_tag else true_tag\n",
    "            \n",
    "            if entity_type not in entity_total:\n",
    "                entity_total[entity_type] = 0\n",
    "                entity_correct[entity_type] = 0\n",
    "                \n",
    "            entity_total[entity_type] += 1\n",
    "            if pred_tag == true_tag:\n",
    "                entity_correct[entity_type] += 1\n",
    "    \n",
    "    # Print per-entity accuracy\n",
    "    print(\"\\nPer-entity accuracy:\")\n",
    "    for entity_type in entity_total:\n",
    "        acc = entity_correct[entity_type] / entity_total[entity_type]\n",
    "        print(f\"{entity_type}: {acc:.4f} ({entity_correct[entity_type]}/{entity_total[entity_type]})\")\n",
    "\n",
    "# Step 7: Save the model\n",
    "model.save_pretrained(\"pii_ner_model\")\n",
    "tokenizer.save_pretrained(\"pii_ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('humannative': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013a1e21ceb4331b7c15f8cbcc3f37fb974b151a523b1f3ac0d85fdf44e83a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
