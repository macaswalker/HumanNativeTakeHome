{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-Home Assignment - Human Native - Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are the ML team at Human Native. We want to build a system that reduces the workload of the operations team: this team curretly reviews hundreds of thousand of media (multimedia: text, image, audio, video, animation).\n",
    "\n",
    "The engineering team currently has a set up like so: \n",
    "\n",
    "```\n",
    "Dataset(org_id, id, name, type)\n",
    "Data(dataset_id, id, value, flag)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a model that automatically flags data that might be in violation.\n",
    "\n",
    "We are doing this to be able to provide a service that will allow consumrs to provide structured information about where, why and how the data is in violation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, what this small MVP project wishes to show is that:\n",
    "1. for a specific modality of data (we will use text)\n",
    "2. we can return structured information about where, why and how the data is in violation\n",
    "3. we want to build a model that does this\n",
    "4. we also want to provide recommendations about the current data structures used by the engineering team to make this problem as easy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some specifications:\n",
    "- we will note our assumptions as we go along\n",
    "- we want to build our own model, where we can\n",
    "- we don't need to worry about persistence\n",
    "- our model currently has access to all previously flagged data items as well as those verified as corrct by the operations team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Plan \n",
    "\n",
    "We will design a pipeline that is independent of the model we use to classify. We will do this to rapidly iterate. We will then train our own distilbert model for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP of an MVP\n",
    "\n",
    "Here, we will simply take in a bunch of text of we want to build something that returns an object: this object contains the original text, the text with redactions, structured information about where the PII is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8m/mlfdjz4d5bd1hd67c6nkhb_00000gn/T/ipykernel_873/2346554238.py\", line 2, in <module>\n",
      "    from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/__init__.py\", line 9, in <module>\n",
      "    from presidio_analyzer.entity_recognizer import EntityRecognizer\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/entity_recognizer.py\", line 6, in <module>\n",
      "    from presidio_analyzer.nlp_engine import NlpArtifacts\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/nlp_engine/__init__.py\", line 4, in <module>\n",
      "    from .nlp_artifacts import NlpArtifacts\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/presidio_analyzer/nlp_engine/nlp_artifacts.py\", line 4, in <module>\n",
      "    from spacy.tokens import Doc, Span\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: LOCATION, start: 80, end: 97, score: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Relevant imports\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "sample_text = \"hello my name is  Mac Walker, my email is macskyewalker@gmail.com and I live at 10 downing street\"\n",
    "sample_entities = [\"PHONE_NUMBER\", \"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"UK_NINO\"]\n",
    "\n",
    "# Please see a full list of supported entities here: (https://microsoft.github.io/presidio/supported_entities/)\n",
    "\n",
    "\n",
    "analyser = AnalyzerEngine()\n",
    "\n",
    "analyser_results = analyser.analyze(text=sample_text, entities = sample_entities, language='en')\n",
    "\n",
    "print((analyser_results[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we now have an extremely basic system that can detect PII. Right now, this is a tool, so lets build functionality around it to actually go through our datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: EMAIL_ADDRESS, start: 42, end: 65, score: 1.0, type: PERSON, start: 18, end: 28, score: 0.85, type: LOCATION, start: 80, end: 97, score: 0.85]\n"
     ]
    }
   ],
   "source": [
    "analyser = AnalyzerEngine()\n",
    "\n",
    "def presidio_detect_PII(text, entities, language):\n",
    "\n",
    "    assert isinstance(text, str), \"text must be a string\"\n",
    "    assert isinstance(entities, list) and all(isinstance(e, str) for e in entities), \"entities must be a list of strings\"\n",
    "    assert isinstance(language, str), \"language must be a string\"\n",
    "\n",
    "    analyser_results = analyser.analyze(text=text, entities=entities, language=language)\n",
    "    return analyser_results\n",
    "\n",
    "# Small Test\n",
    "sample_results = presidio_detect_PII(text= sample_text, entities = sample_entities, language = 'en')\n",
    "print(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: hello my name is  [PERSON], my email is [EMAIL_ADDRESS] and I live at [LOCATION]\n",
      "items:\n",
      "[\n",
      "    {'start': 70, 'end': 80, 'entity_type': 'LOCATION', 'text': '[LOCATION]', 'operator': 'replace'},\n",
      "    {'start': 40, 'end': 55, 'entity_type': 'EMAIL_ADDRESS', 'text': '[EMAIL_ADDRESS]', 'operator': 'replace'},\n",
      "    {'start': 18, 'end': 26, 'entity_type': 'PERSON', 'text': '[PERSON]', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anonymiser = AnonymizerEngine()\n",
    "def presidio_replace_PII(text, entities, language):\n",
    "\n",
    "    analyzer_results = presidio_detect_PII(text = text, entities = entities,language=language)\n",
    "\n",
    "    operators = {\n",
    "            result.entity_type: OperatorConfig(\"replace\", {\"new_value\": f\"[{result.entity_type}]\"}) for result in analyser_results\n",
    "        } # got this from ChatGPT\n",
    "\n",
    "    anonymised_results = anonymiser.anonymize(\n",
    "        text = text,\n",
    "        analyzer_results = analyzer_results,\n",
    "        operators = operators\n",
    "        \n",
    "    )\n",
    "    return anonymised_results\n",
    "\n",
    "#Small Test\n",
    "sample_anonymised = presidio_replace_PII(text= sample_text, entities = sample_entities, language = 'en')\n",
    "print(sample_anonymised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we have some functions that allow us to take in a piece of text, detect where the PII is, and return the text that has been anonymised.\n",
    "\n",
    "Right now, we want to test two models. We have our approach above which isn't ours, and a few more ML approaches for models we can train. \n",
    "\n",
    "The next step, before training our model, should be construction of a relatively big dataset (1000 blog post entries), so we can start with our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faker\n",
    "\n",
    "def generate_blogpost(pii_mode):\n",
    "    base_sentences = faker.paragraph(nb_sentences=6).split('. ')\n",
    "    flag = 1 if pii_mode != \"none\" else 0\n",
    "    pii_metadata = []\n",
    "\n",
    "    pii_snippets = [\n",
    "        (\"NAME\", faker.name),\n",
    "        (\"EMAIL\", faker.email),\n",
    "        (\"LOCATION\", faker.city),\n",
    "        (\"PHONE\", faker.phone_number),\n",
    "        (\"URL\", lambda: f\"https://{faker.domain_name()}\"),\n",
    "    ]\n",
    "\n",
    "    if pii_mode == \"none\":\n",
    "        full_text = '. '.join(base_sentences).strip()\n",
    "        return full_text, flag, pii_mode, pii_metadata\n",
    "\n",
    "    pii_type, pii_func = random.choice(pii_snippets)\n",
    "    pii_value = pii_func()\n",
    "\n",
    "    if pii_mode == \"standalone\":\n",
    "        insert_idx = random.randint(0, len(base_sentences))\n",
    "        base_sentences.insert(insert_idx, pii_value)\n",
    "\n",
    "    elif pii_mode == \"raw\":\n",
    "        idx = random.randint(0, len(base_sentences) - 1)\n",
    "        base_sentences[idx] += f\". {pii_value}\"\n",
    "\n",
    "    elif pii_mode == \"embedded\":\n",
    "        embedded_templates = [\n",
    "            (\"NAME\", f\"my name is {faker.name()}\"),\n",
    "            (\"EMAIL\", f\"you can email me at {faker.email()}\"),\n",
    "            (\"LOCATION\", f\"I live in {faker.city()}\"),\n",
    "            (\"PHONE\", f\"my number is {faker.phone_number()}\"),\n",
    "            (\"URL\", f\"visit my site at https://{faker.domain_name()}\"),\n",
    "        ]\n",
    "        pii_type, pii_text = random.choice(embedded_templates)\n",
    "        idx = random.randint(0, len(base_sentences) - 1)\n",
    "        if random.random() < 0.5:\n",
    "            base_sentences[idx] = pii_text + '. ' + base_sentences[idx]\n",
    "        else:\n",
    "            base_sentences[idx] += '. ' + pii_text\n",
    "        pii_value = pii_text  # overwrite for consistency\n",
    "\n",
    "    # Join final text and record location of the PII span\n",
    "    full_text = '. '.join(base_sentences).strip()\n",
    "\n",
    "    if pii_value in full_text:\n",
    "        start = full_text.index(pii_value)\n",
    "        end = start + len(pii_value)\n",
    "        pii_metadata.append({\n",
    "            \"type\": pii_type,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"value\": pii_value\n",
    "        })\n",
    "\n",
    "    return full_text, flag, pii_mode, pii_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the above was created using ChatGPT - it did this in like 5 minutes of discussion! This is great though. Let's go over what we currently have:\n",
    "\n",
    "Dataset(ord_ig, id, name, type): I haven't 'created' this yet, because we basically only have 1 entry to it (which is what we will be using to train our model). \n",
    " - ```Dataset(faker_id: (haven't made one), faker_blog_posts_id : a5c85cbf-c960-431f-b2e5-d4ba1a9601b9, faker_blog_posts, text)```\n",
    "\n",
    "\n",
    "Data(dataset_id, id, value, flag):\n",
    "- ```Data(a5c85cbf-c960-431f-b2e5-d4ba1a9601b9,bbb0d0ec-ec58-4f68-8047-692a59edfd44, \"Value idea trade left. Bit practice already billion call degree. Tax professor mission stock because. Ahead each fish onto.\", 0, none)```\n",
    "\n",
    "So we have an example of our Data and our Dataset. Let us make some observations about our data. There are a few.\n",
    "\n",
    "---------- \n",
    "\n",
    "1. the naming of ```org_id, id, dataset_id and id``` (again?!!!) is confusing to me. Hence, I recommend changing the id naming convention to maintain readability, to this: \n",
    "- ```Dataset(org_id, id, name, type)``` -> ```Dataset(org_id, dataset_id, name, type)```\n",
    "- ```Data(dataset_id, id, value, flag)``` -> ```Data(dataset_id, data_id, value, flag)```\n",
    "\n",
    "2. We have the value, which is basically the data values (text, images, animation etc). We also have a flag which supposedly tells us whether it contains PII? We should actually include WHERE in the content the PII is, WHAT the PII is (what has been detected) and HOW we have dealt with it (have we removed? anonymised? encrypted?)\n",
    "\n",
    "3. We currently have a flag. But where is that flag from? Is it from the operations team who have flagged this? or is it from our model? we should keep track of which model gave it whcih flag so we can continuously check against the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 blog posts\n",
    "dataset_id = str(uuid4())\n",
    "data_rows = []\n",
    "\n",
    "for mode in [\"embedded\", \"raw\", \"standalone\", \"none\"]:\n",
    "    for _ in range(250):\n",
    "        text, flag, pii_mode, pii_spans = generate_blogpost(pii_mode=mode)\n",
    "        data_rows.append({\n",
    "            'dataset_id': dataset_id,\n",
    "            'id': str(uuid4()),\n",
    "            'value': text,\n",
    "            'flag': flag,\n",
    "            'pii_mode': pii_mode,\n",
    "            'pii_spans': json.dumps(pii_spans)\n",
    "        })\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(data_rows)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(\"Data_with_spans.csv\", index=False)\n",
    "df.to_json(\"Data_with_spans.json\", orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting PII with Presidio: 100%|██████████| 1000/1000 [00:19<00:00, 50.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 paragraphs: 19.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans.csv\")\n",
    "\n",
    "sample_entities = [\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\", \"LOCATION\", \"URL\"]\n",
    "\n",
    "analyser = AnalyzerEngine()\n",
    "\n",
    "start = time.time()\n",
    "found_flags = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Detecting PII with Presidio\"):\n",
    "    text = row[\"value\"]\n",
    "    results = analyser.analyze(text=text, entities=sample_entities, language=\"en\")\n",
    "    found_flags.append(int(len(results) > 0))\n",
    "\n",
    "df[\"found_flag\"] = found_flags\n",
    "df.to_csv(\"Data_with_spans_and_found_flag.csv\", index=False)\n",
    "\n",
    "print(f\"Time taken for 1000 paragraphs: {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      no_pii       0.80      0.96      0.87       250\n",
      "     has_pii       0.99      0.92      0.95       750\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.89      0.94      0.91      1000\n",
      "weighted avg       0.94      0.93      0.93      1000\n",
      "\n",
      "\n",
      "🔄 Confusion Matrix:\n",
      "[[240  10]\n",
      " [ 60 690]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans_and_found_flag.csv\")\n",
    "\n",
    "print(\"\\n📊 Overall Classification Report:\")\n",
    "print(classification_report(df[\"flag\"], df[\"found_flag\"], target_names=[\"no_pii\", \"has_pii\"]))\n",
    "\n",
    "print(\"\\n🔄 Confusion Matrix:\")\n",
    "print(confusion_matrix(df[\"flag\"], df[\"found_flag\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Accuracy by PII Mode:\n",
      "\n",
      "Embedded    : Accuracy = 0.944 (250 samples)\n",
      "None        : Accuracy = 0.960 (250 samples)\n",
      "Raw         : Accuracy = 0.920 (250 samples)\n",
      "Standalone  : Accuracy = 0.896 (250 samples)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"Data_with_spans_and_found_flag.csv\")\n",
    "\n",
    "print(\"\\n📁 Accuracy by PII Mode:\\n\")\n",
    "\n",
    "for mode in sorted(df[\"pii_mode\"].unique()):\n",
    "    subset = df[df[\"pii_mode\"] == mode]\n",
    "    y_true = subset[\"flag\"]\n",
    "    y_pred = subset[\"found_flag\"]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"{mode.title():<12}: Accuracy = {acc:.3f} ({len(subset)} samples)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so lets think about what we have - a _baseline_.\n",
    "\n",
    "This, in my opinion, is a really useful thing to have. It is all well and good fine-tuning a huge transformer model for PII detection (which we will proceed to do), but we should understand _why_ we are doing it and what benefits it will give this process.\n",
    "\n",
    "Our Presidio-only model (which actually uses a transformer sneakily in the back) achieves an accuracy of 92%. This is okay - not amazing. \n",
    "When inspecing further, we can see a difference between the accuracies of the four different types of PII injection we used to create the dummy dataset. Our sample size is too small to make too many conclusions, apart from we can maybe say that the model is better at classifying `None` and `Embedded` relative to `Raw` and `Standalone`. \n",
    "\n",
    "I posit that the reason for this is that for both `None` and `Embedded`, these are more \n",
    "\n",
    "\n",
    "FINISH LATER\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('humannative': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013a1e21ceb4331b7c15f8cbcc3f37fb974b151a523b1f3ac0d85fdf44e83a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
