{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-Home Assignment - PII Detection\n",
    "\n",
    "Human Native's operations team currently reviews huge volumes of content for Personally Identifiable Information (PII), and the amount ot review is growing quickly. We need to be able to automate this system robustly!\n",
    "\n",
    "This notebook walks through:\n",
    "- Establishing a _baseline_ - incredibly important in all ML tasks. We discuss contemporary methods that already exist (services, Presidio) to inspire our model choice.\n",
    "- Training a custom transformer for token-level PII detection\n",
    "- Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions before beginning\n",
    "\n",
    "- We will focus on textual data\n",
    "- We will only focus on these PII entities:\n",
    "    - \"NAME\"\n",
    "    - \"EMAIL\"\n",
    "    - \"LOCATION\"\n",
    "    - \"PHONE\"\n",
    "    - \"URL\"\n",
    "- We want to classify WORDS (and therefore tokens) as containing PII, as opposed to whether a small chunk of text contains PII _somewhere_ within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a Baseline\n",
    "\n",
    "Microsoft Presidio (https://microsoft.github.io/presidio/) is an awesome SDK that provides PII detection in a really use to way!\n",
    "\n",
    "We have written some small functions that make it super simple to use the functionality of presidio (see `src\\presidio`).\n",
    "\n",
    "Below, we see examples of Presidio Detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: EMAIL_ADDRESS, start: 29, end: 48, score: 1.0\n",
      "type: PERSON, start: 8, end: 18, score: 0.85\n",
      "type: URL, start: 33, end: 48, score: 0.5\n",
      "type: PHONE_NUMBER, start: 67, end: 79, score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from src.presidio.detector import presidio_detect\n",
    "\n",
    "sample_text = \"Hi, I'm Mac Walker. Contact: mac@humannative.com (hopefully (; ?), 07123 456789\"\n",
    "result = presidio_detect(sample_text)\n",
    "for results in result:\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, as we can see, Presidio takes in a string of text and returns the token level spans of WHERE  the PII infraction takes place, and WHAT kind of infraction it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore want to be able to test this data. We generate dummy data (please see ```src/data``` for dataset generation).\n",
    "\n",
    "Each generated entry represents a blog post and contains one of several types of PII (given earlier) inserted in different formats (embedded, raw, standalone, or none). The original dataset is saved with the following columns:\n",
    "\n",
    "```csv\n",
    "dataset_id, data_id, value, flag, pii_mode, pii_spans\n",
    "```\n",
    "\n",
    "The different formats of PII insertion are:\n",
    "\n",
    "- Embedded: PII is integrated naturally into a sentence to simulate real-world usage.\n",
    "  Example: \"Hi, my name is John Smith and I live in New York.\"\n",
    "\n",
    "- Raw: PII is appended to an otherwise unrelated piece of text without much context.\n",
    "  Example: \"The project launched last year john.smith@example.com\"\n",
    "\n",
    "- Standalone: The PII appears as a separate sentence or fragment, disconnected from any surrounding text.\n",
    "  Example: \"john.smith@example.com. The project was completed last year.\"\n",
    "\n",
    "- None: The text contains no PII at all. This serves as a control group to test false positives.\n",
    "  Example: \"The project deadline was moved to next Thursday due to client feedback.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we had different insertion techniques for our data generation was because of the nature of transformer models. As we are relying on ```faker```-generated data, we want to embed PII in a way that closely resembles real-world usage. Hence, using actual word and sentences that make sense (e.g. I live in New York). But, we did not want the model to just pick up on spurious relationships (e.g., predicting PII on any tokens after the string \"i live in\").\n",
    "I hope that the diversity in PII injection styles ensures that the model learns to detect PII entities based on their actual semantic and syntactic charachteristics, as opposed to their data-generated phrasing context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us, therefore, inspect Presidio's accuracy on our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ Total prediction time: 27.11 seconds\n",
      "⏱️ Average time per sample: 0.0271 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from src.presidio.detector import presidio_detect\n",
    "\n",
    "df = pd.read_csv(\"data/test/test_Data.csv\")\n",
    "\n",
    "presidio_flags = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"value\"]\n",
    "    try:\n",
    "        results = presidio_detect(text)\n",
    "        presidio_flags.append(int(len(results) > 0))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {i}: {e}\")\n",
    "        presidio_flags.append(0)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n⏱️ Total prediction time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"⏱️ Average time per sample: {(end_time - start_time) / len(df):.4f} seconds\")\n",
    "\n",
    "df[\"default_flag\"] = presidio_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Default Presidio Classification Report on Test Dataset:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      no_pii       0.77      0.96      0.86       250\n",
      "     has_pii       0.99      0.90      0.94       750\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.88      0.93      0.90      1000\n",
      "weighted avg       0.93      0.92      0.92      1000\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[241   9]\n",
      " [ 72 678]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Default Presidio Classification Report on Test Dataset:\\n\")\n",
    "print(classification_report(df[\"flag\"], df[\"default_flag\"], target_names=[\"no_pii\", \"has_pii\"]))\n",
    "\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "print(confusion_matrix(df[\"flag\"], df[\"default_flag\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have a good set of results to check against. \n",
    "- 241 TN \n",
    "- 678 TP\n",
    "- 9 FP\n",
    "- 72 FN - This is not good!!!\n",
    "\n",
    "Our recall is 90% - we should really want to increase this! \n",
    "\n",
    "In our case, we really want to mitigate against FN. It is _much_ better to say something is PII when it isn't than to not flag actual PII:\n",
    "\n",
    "1. Flag PII that isn't actually PII -> slightly reduce the information contained within the training dataset we provide to AI labs\n",
    "2. Don't flag PII that is actually PII -> possible legal culpability!\n",
    "\n",
    "Furthermore, our classification results only check whether our model detects PII in a piece of text, not whether we EXACTLY capture where the PII is. This is something to be improved upon in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inspires a new way. Luckily, Presidio provides a framework for integrating your own built in model to provide the 'NLP backbone' of PII detection.\n",
    "\n",
    "I have trained a model (see ```src/training```) to do exactly this!. We use DistilBertforNER from huggingface, finetuning on the data we created in ```data/Data.csv```.\n",
    "\n",
    "Let us inspect its performance underneath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Documents/Career/Interviews/HumanNative/humannative/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating Token Classifier: 100%|██████████| 1000/1000 [02:34<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ Total prediction time: 154.54 seconds\n",
      "⏱️ Average time per sample: 0.1545 seconds\n",
      "\n",
      "📊 Token Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      no_pii       1.00      1.00      1.00       250\n",
      "     has_pii       1.00      1.00      1.00       750\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "🔄 Confusion Matrix:\n",
      "[[250   0]\n",
      " [  0 750]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.token_classifier import load_bert_token_model, evaluate_on_dataframe\n",
    "\n",
    "df = pd.read_csv(\"data/test/test_Data.csv\")\n",
    "tokenizer, model = load_bert_token_model()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df = evaluate_on_dataframe(df, tokenizer, model)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "avg_time = total_time / len(df)\n",
    "\n",
    "print(f\"\\n⏱️ Total prediction time: {total_time:.2f} seconds\")\n",
    "print(f\"⏱️ Average time per sample: {avg_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\n📊 Token Model Classification Report:\")\n",
    "print(classification_report(df[\"flag\"], df[\"token_model_flag\"], target_names=[\"no_pii\", \"has_pii\"]))\n",
    "\n",
    "print(\"\\n🔄 Confusion Matrix:\")\n",
    "print(confusion_matrix(df[\"flag\"], df[\"token_model_flag\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this model is obviously quite good! \n",
    "\n",
    "That being said - we should not be overly optimistic. The structure of the toy data probably lends itself to this task. It would be much better to test it on  real world data that has been assessed with humans in the loop - e.g. the data from the operations team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('humannative': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013a1e21ceb4331b7c15f8cbcc3f37fb974b151a523b1f3ac0d85fdf44e83a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
